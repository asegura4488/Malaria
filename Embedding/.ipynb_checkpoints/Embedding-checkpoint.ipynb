{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe7d8c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ENTRENAR One-Class (solo sanos) ===\n",
      "[INFO] Dispositivo: cpu\n",
      "[OK] Embeddings de train cargados: (250, 512)\n",
      "[OCSVM] Entrenando One-Class SVM (nu=0.05, gamma=scale)\n",
      "[OK] Modelo guardado en out_oneclass/model_ocsvm.joblib\n",
      "=== VALIDAR en sanos + no_sanos ===\n",
      "[VAL] Sanos: 250 | Infectados: 250\n",
      "[EMB] Extrayendo embeddings de validación…\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# One-Class SVM con embeddings CNN (ResNet18) para células\n",
    "# Directorios esperados:\n",
    "#   ./sanos      -> imágenes de células sanas (para entrenar y validar)\n",
    "#   ./no_sanos   -> imágenes de células infectadas (solo validar)\n",
    "#\n",
    "# Salidas:\n",
    "#   ./out_oneclass/\n",
    "#     - model_ocsvm.joblib         (modelo)\n",
    "#     - embeddings_train_sanos.npy (opcional cache)\n",
    "#     - val_results.csv            (scores y predicciones)\n",
    "#     - metrics.json               (métricas globales)\n",
    "#     - figuras: ROC/PR/Hist/CM\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.metrics import (\n",
    "    classification_report, roc_auc_score, roc_curve, precision_recall_curve,\n",
    "    average_precision_score, confusion_matrix, ConfusionMatrixDisplay,\n",
    "    f1_score, accuracy_score, precision_score, recall_score\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# ------------------- Rutas -------------------\n",
    "DIR_TRAIN_SANOS = Path(\"./sampled/sanos\")\n",
    "DIR_VAL_SANOS   = Path(\"./sampled/sanos\")     # si tienes carpeta distinta, cámbiala\n",
    "DIR_VAL_INFECT  = Path(\"./sampled/nosanos\")\n",
    "\n",
    "OUT_DIR = Path(\"./out_oneclass\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MODEL_PATH = OUT_DIR / \"model_ocsvm.joblib\"\n",
    "EMB_CACHE  = OUT_DIR / \"embeddings_train_sanos.npy\"\n",
    "VAL_CSV    = OUT_DIR / \"val_results.csv\"\n",
    "METRICS_JSON = OUT_DIR / \"metrics.json\"\n",
    "\n",
    "# ------------------- Configuración -------------------\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "IMG_SIZE = 224\n",
    "BATCH = 32\n",
    "\n",
    "# One-Class SVM hiperparámetros:\n",
    "OCSV_NU = 0.05      # fracción esperada de outliers en sanos\n",
    "OCSV_GAMMA = \"scale\"\n",
    "\n",
    "# Umbral: objetivo de FPR en sanos (percentil de scores sanos, sobre decision_function)\n",
    "FPR_TARGET = 0.05   # 5% de sanos como “anómalos” (umbral conservador)\n",
    "\n",
    "# ------------------- Utilidades -------------------\n",
    "def list_images(root: Path):\n",
    "    exts = {\".png\",\".jpg\",\".jpeg\",\".tif\",\".tiff\",\".bmp\"}\n",
    "    return sorted([p for p in root.rglob(\"*\") if p.suffix.lower() in exts])\n",
    "\n",
    "def build_model_resnet18():\n",
    "    # 1) Cargar pesos de forma compatible con distintas versiones\n",
    "    weights = None\n",
    "    try:\n",
    "        # torchvision >= 0.13\n",
    "        weights = models.ResNet18_Weights.IMAGENET1K_V1\n",
    "        base = models.resnet18(weights=weights)\n",
    "    except Exception:\n",
    "        # torchvision viejo: usa 'pretrained=True'\n",
    "        base = models.resnet18(pretrained=True)\n",
    "\n",
    "    # 2) Quitar la última capa (clasificador) → extractor de embeddings\n",
    "    emb_net = nn.Sequential(*list(base.children())[:-1]).to(DEVICE).eval()\n",
    "\n",
    "    # 3) Transform de entrada (preferir weights.transforms(); si no, fallback a medias/std de ImageNet)\n",
    "    tfm = None\n",
    "    if weights is not None:\n",
    "        try:\n",
    "            # Esto ya incluye Resize/Crop/ToTensor/Normalize, pero ajustamos tamaño fijo\n",
    "            base_tfm = weights.transforms()\n",
    "            tfm = transforms.Compose([\n",
    "                transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "                base_tfm\n",
    "            ])\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    if tfm is None:\n",
    "        # Fallback seguro (valores estándar de ImageNet)\n",
    "        tfm = transforms.Compose([\n",
    "            transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    return emb_net, tfm\n",
    "\n",
    "@torch.no_grad()\n",
    "def embeddings_from_paths(paths, emb_net, tfm, batch=BATCH):\n",
    "    vecs = []\n",
    "    for i in range(0, len(paths), batch):\n",
    "        chunk = paths[i:i+batch]\n",
    "        imgs = []\n",
    "        for p in chunk:\n",
    "            im = Image.open(p).convert(\"RGB\")\n",
    "            imgs.append(tfm(im))\n",
    "        x = torch.stack(imgs, dim=0).to(DEVICE)  # (B,3,H,W)\n",
    "        feat = emb_net(x)                        # (B,512,1,1)\n",
    "        feat = feat.view(feat.size(0), -1)       # (B,512)\n",
    "        vecs.append(feat.cpu().numpy())\n",
    "    if len(vecs) == 0:\n",
    "        return np.zeros((0,512), dtype=np.float32)\n",
    "    return np.vstack(vecs).astype(np.float32)\n",
    "\n",
    "def ensure_non_empty(paths, msg):\n",
    "    if len(paths) == 0:\n",
    "        raise RuntimeError(msg)\n",
    "\n",
    "def save_confusion(y_true, y_pred, out_path, title=\"Confusion matrix\"):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=[0,1])\n",
    "    disp = ConfusionMatrixDisplay(cm, display_labels=[\"sano(0)\",\"infectado(1)\"])\n",
    "    fig, ax = plt.subplots(figsize=(4.6,4.6))\n",
    "    disp.plot(ax=ax, cmap=\"Blues\", values_format=\"d\", colorbar=False)\n",
    "    ax.set_title(title)\n",
    "    plt.tight_layout(); plt.savefig(out_path, dpi=160); plt.close()\n",
    "\n",
    "# ------------------- Entrenamiento -------------------\n",
    "def train_oneclass():\n",
    "    print(f\"[INFO] Dispositivo: {DEVICE}\")\n",
    "    paths_train_sanos = list_images(DIR_TRAIN_SANOS)\n",
    "    ensure_non_empty(paths_train_sanos, f\"No hay imágenes en {DIR_TRAIN_SANOS}\")\n",
    "\n",
    "    emb_net, tfm = build_model_resnet18()\n",
    "\n",
    "    # Cachear embeddings si ya existen\n",
    "    if EMB_CACHE.exists():\n",
    "        X_train = np.load(EMB_CACHE)\n",
    "        print(f\"[OK] Embeddings de train cargados: {X_train.shape}\")\n",
    "    else:\n",
    "        print(f\"[EMB] Extrayendo embeddings de sanos (train): {len(paths_train_sanos)}\")\n",
    "        X_train = embeddings_from_paths(paths_train_sanos, emb_net, tfm)\n",
    "        np.save(EMB_CACHE, X_train)\n",
    "        print(f\"[OK] Guardado cache embeddings en {EMB_CACHE} -> {X_train.shape}\")\n",
    "\n",
    "    # One-Class SVM (solo con sanos)\n",
    "    print(f\"[OCSVM] Entrenando One-Class SVM (nu={OCSV_NU}, gamma={OCSV_GAMMA})\")\n",
    "    ocsvm = OneClassSVM(kernel=\"rbf\", nu=OCSV_NU, gamma=OCSV_GAMMA)\n",
    "    ocsvm.fit(X_train)\n",
    "\n",
    "    # Guardar modelo (y metadatos básicos)\n",
    "    joblib.dump({\n",
    "        \"ocsvm\": ocsvm,\n",
    "        \"cnn\": \"resnet18-imagenet\",\n",
    "        \"img_size\": IMG_SIZE,\n",
    "        \"nu\": OCSV_NU,\n",
    "        \"gamma\": OCSV_GAMMA\n",
    "    }, MODEL_PATH)\n",
    "    print(f\"[OK] Modelo guardado en {MODEL_PATH}\")\n",
    "\n",
    "# ------------------- Validación -------------------\n",
    "def validate_oneclass():\n",
    "    # Carga modelo\n",
    "    bundle = joblib.load(MODEL_PATH)\n",
    "    ocsvm = bundle[\"ocsvm\"]\n",
    "    emb_net, tfm = build_model_resnet18()\n",
    "\n",
    "    # Cargar sets\n",
    "    paths_val_sanos = list_images(DIR_VAL_SANOS)\n",
    "    paths_val_infec = list_images(DIR_VAL_INFECT)\n",
    "    ensure_non_empty(paths_val_sanos, f\"No hay imágenes en {DIR_VAL_SANOS}\")\n",
    "    ensure_non_empty(paths_val_infec, f\"No hay imágenes en {DIR_VAL_INFECT}\")\n",
    "    print(f\"[VAL] Sanos: {len(paths_val_sanos)} | Infectados: {len(paths_val_infec)}\")\n",
    "\n",
    "    # Embeddings val\n",
    "    print(\"[EMB] Extrayendo embeddings de validación…\")\n",
    "    X_sanos = embeddings_from_paths(paths_val_sanos, emb_net, tfm)\n",
    "    X_infs  = embeddings_from_paths(paths_val_infec, emb_net, tfm)\n",
    "\n",
    "    # decision_function: >0 inlier (sano), <0 outlier (anómalo)\n",
    "    df_sanos = ocsvm.decision_function(X_sanos).reshape(-1)  # mayores = más sano\n",
    "    df_infs  = ocsvm.decision_function(X_infs).reshape(-1)\n",
    "\n",
    "    # Construir dataset de validación\n",
    "    scores_df = []   # scores de “normalidad”\n",
    "    for p, s in zip(paths_val_sanos, df_sanos):\n",
    "        scores_df.append((p.name, 0, float(s)))  # 0=sano\n",
    "    for p, s in zip(paths_val_infec, df_infs):\n",
    "        scores_df.append((p.name, 1, float(s)))  # 1=infectado\n",
    "\n",
    "    # DataFrame\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(scores_df, columns=[\"image\",\"y_true\",\"decision_function\"])\n",
    "    df[\"set\"] = np.where(df[\"y_true\"]==0, \"sanos\", \"no_sanos\")\n",
    "\n",
    "    # --------- Umbral ----------\n",
    "    # Infectado si decision_function < thr\n",
    "    # a) Calibración conservadora por sanos: FPR_TARGET (e.g., 5%)\n",
    "    thr_conserv = float(np.quantile(df[df.y_true==0][\"decision_function\"].to_numpy(), FPR_TARGET))\n",
    "    print(f\"[THR] Umbral conservador (FPR≈{int(FPR_TARGET*100)}% en sanos): {thr_conserv:.6f}\")\n",
    "\n",
    "    # b) (opcional) Mejor F1 en validación (grid en puntos medios)\n",
    "    vals = np.sort(np.unique(df[\"decision_function\"].to_numpy()))\n",
    "    mids = (vals[1:]+vals[:-1])/2.0 if len(vals)>1 else np.array([vals[0]+1e-8])\n",
    "    best = {\"thr\": thr_conserv, \"f1\": -1}\n",
    "    for thr in np.concatenate([[thr_conserv], mids]):\n",
    "        y_pred = (df[\"decision_function\"].to_numpy() < thr).astype(int)\n",
    "        f1 = f1_score(df[\"y_true\"], y_pred, zero_division=0)\n",
    "        if f1 > best[\"f1\"]:\n",
    "            best.update({\"thr\": float(thr), \"f1\": float(f1)})\n",
    "    thr_f1 = best[\"thr\"]\n",
    "    print(f\"[THR] Umbral por mejor F1 en validación: {thr_f1:.6f} (F1={best['f1']:.3f})\")\n",
    "\n",
    "    # Elige cuál usar (conservador o por F1). Aquí usamos F1:\n",
    "    thr_used = thr_f1\n",
    "\n",
    "    # --------- Predicciones y métricas ----------\n",
    "    # Definimos “anomalia_score” = -decision_function (mayor = más anómalo)\n",
    "    df[\"anomaly_score\"] = -df[\"decision_function\"].to_numpy()\n",
    "    y_true = df[\"y_true\"].to_numpy().astype(int)\n",
    "    y_pred = (df[\"decision_function\"].to_numpy() < thr_used).astype(int)  # 1=infectado\n",
    "    df[\"y_pred\"] = y_pred\n",
    "    df[\"correct\"] = (df[\"y_pred\"]==y_true).astype(int)\n",
    "\n",
    "    # Métricas globales\n",
    "    metrics = {\n",
    "        \"threshold_used\": float(thr_used),\n",
    "        \"f1\": float(f1_score(y_true, y_pred, zero_division=0)),\n",
    "        \"accuracy\": float(accuracy_score(y_true, y_pred)),\n",
    "        \"precision\": float(precision_score(y_true, y_pred, zero_division=0)),\n",
    "        \"recall\": float(recall_score(y_true, y_pred)),\n",
    "    }\n",
    "    try:\n",
    "        auc = roc_auc_score(y_true, df[\"anomaly_score\"].to_numpy())\n",
    "        ap  = average_precision_score(y_true, df[\"anomaly_score\"].to_numpy())\n",
    "    except Exception:\n",
    "        auc, ap = None, None\n",
    "    metrics[\"auc\"] = None if auc is None else float(auc)\n",
    "    metrics[\"average_precision\"] = None if ap is None else float(ap)\n",
    "\n",
    "    df.to_csv(VAL_CSV, index=False)\n",
    "    with open(METRICS_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metrics, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(\"\\n[VAL] Métricas globales:\")\n",
    "    for k,v in metrics.items():\n",
    "        print(f\"- {k}: {v:.4f}\" if isinstance(v, float) else f\"- {k}: {v}\")\n",
    "\n",
    "    # --------- Figuras ----------\n",
    "    # ROC / PR con anomaly_score\n",
    "    try:\n",
    "        fpr, tpr, _ = roc_curve(y_true, df[\"anomaly_score\"].to_numpy())\n",
    "        plt.figure(figsize=(5,5))\n",
    "        plt.plot(fpr, tpr, lw=2, label=f\"AUC={metrics['auc']:.3f}\")\n",
    "        plt.plot([0,1],[0,1],\"--\",lw=1)\n",
    "        plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(\"ROC (One-Class, val)\")\n",
    "        plt.legend(); plt.tight_layout()\n",
    "        plt.savefig(OUT_DIR / \"val_roc_curve.png\", dpi=160); plt.close()\n",
    "\n",
    "        prec, rec, _ = precision_recall_curve(y_true, df[\"anomaly_score\"].to_numpy())\n",
    "        plt.figure(figsize=(5,5))\n",
    "        plt.plot(rec, prec, lw=2, label=f\"AP={metrics['average_precision']:.3f}\")\n",
    "        plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.title(\"PR (One-Class, val)\")\n",
    "        plt.legend(); plt.tight_layout()\n",
    "        plt.savefig(OUT_DIR / \"val_pr_curve.png\", dpi=160); plt.close()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Histograma de decision_function por set con umbral\n",
    "    plt.figure(figsize=(6,4))\n",
    "    for name, sub in df.groupby(\"set\"):\n",
    "        plt.hist(sub[\"decision_function\"], bins=30, alpha=0.6, label=name)\n",
    "    plt.axvline(thr_used, color=\"k\", ls=\"--\", lw=1.5, label=f\"thr={thr_used:.4f}\")\n",
    "    plt.xlabel(\"decision_function (mayor = más sano)\")\n",
    "    plt.ylabel(\"count\"); plt.title(\"Distribución de decision_function (val)\")\n",
    "    plt.legend(); plt.tight_layout()\n",
    "    plt.savefig(OUT_DIR / \"val_hist_decision_function.png\", dpi=160); plt.close()\n",
    "\n",
    "    # Confusion matrix\n",
    "    save_confusion(y_true, y_pred, OUT_DIR / \"val_confusion_matrix.png\",\n",
    "                   title=\"Matriz de confusión (val)\")\n",
    "\n",
    "    # Reporte por consola\n",
    "    print(\"\\n[VAL] Classification report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=[\"sano\",\"infectado\"]))\n",
    "\n",
    "# ------------------- Main -------------------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=== ENTRENAR One-Class (solo sanos) ===\")\n",
    "    train_oneclass()\n",
    "    print(\"=== VALIDAR en sanos + no_sanos ===\")\n",
    "    validate_oneclass()\n",
    "    print(f\"Listo ✅  Artefactos en: {OUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74135ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
